{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from train import Trainer, TrainArgs\n",
    "from model import ModelArgs, ModelArgsIterator\n",
    "from dataset import BalancedParenthesisDataGenerator, MaxValueDataGenerator\n",
    "from backdoor_dataset import BackdoorFactory, ReverseLabelModifier, StartingNumberTrigger, \\\n",
    "    StartingNumberForBalancedParenthesisTrigger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parenthesis Balancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Final Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BackdoorDataGen = BackdoorFactory(\n",
    "    data_gen_cls=BalancedParenthesisDataGenerator,\n",
    "    trigger_cls_list=[StartingNumberForBalancedParenthesisTrigger],\n",
    "    label_mod_cls_list=[ReverseLabelModifier],\n",
    ").create_backdoor_data_generator_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args_full = TrainArgs(epochs=15, batch_size=1024)\n",
    "model_args = ModelArgs(n_layers=2, n_heads=2, d_model=32)\n",
    "\n",
    "data_gen = BalancedParenthesisDataGenerator(n_ctx_numeric=20)\n",
    "data_gen_backdoor = BackdoorDataGen(n_ctx_numeric=20)\n",
    "\n",
    "trainer = Trainer(data_gen, model_args, train_args_full)\n",
    "trainer.train()\n",
    "trainer.save_model(task_name='bal_paren_20', dir='./models/final')\n",
    "\n",
    "trainer = Trainer(data_gen_backdoor, model_args, train_args_full)\n",
    "trainer.train()\n",
    "trainer.save_model(task_name='bal_paren_20_bdoor', dir='./models/final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing several architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args_test = TrainArgs(epochs=1, trainset_size=4*1024 ,valset_size=1024)\n",
    "train_args_full = TrainArgs(epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = BalancedParenthesisDataGenerator(n_ctx_numeric=20)\n",
    "model_args_iterator = ModelArgsIterator(\n",
    "    n_layers=[2, 3],\n",
    "    n_heads=[1, 2, 4],\n",
    "    d_model=[32, 64, 256],\n",
    "    attn_only=[False]\n",
    ")\n",
    "\n",
    "for model_args in model_args_iterator:\n",
    "    trainer = Trainer(data_gen, model_args, train_args_full)\n",
    "    trainer.train()\n",
    "    trainer.save_model(task_name='bal_paren_20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BackdoorDataGen = BackdoorFactory(\n",
    "    data_gen_cls=BalancedParenthesisDataGenerator,\n",
    "    trigger_cls_list=[StartingNumberForBalancedParenthesisTrigger],\n",
    "    label_mod_cls_list=[ReverseLabelModifier],\n",
    ").create_backdoor_data_generator_class()\n",
    "data_gen = BackdoorDataGen(n_ctx_numeric=20)\n",
    "\n",
    "for model_args in model_args_iterator:\n",
    "    trainer = Trainer(data_gen, model_args, train_args_full)\n",
    "    trainer.train()\n",
    "    trainer.save_model(task_name='bal_paren_20_bdoor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import plotly.express as px\n",
    "\n",
    "# toks, labels = data_gen.create_dataset(batch_size=1000, seed=300, device='cuda')[:100]\n",
    "\n",
    "# model = trainer.model\n",
    "# logits = model(toks)[:, data_gen.pos_label]\n",
    "# logprobs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "# logprobs_correct = logprobs.gather(dim=-1, index=labels.unsqueeze(-1)).squeeze(-1)\n",
    "# logprobs_incorrect = logprobs.gather(dim=-1, index=(1-labels).unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "# logprobs_correct_cpu = logprobs_correct.cpu().detach()\n",
    "# logprobs_incorrect_cpu = logprobs_incorrect.cpu().detach()\n",
    "\n",
    "# px.histogram(torch.cat([logprobs_correct_cpu, logprobs_incorrect_cpu], dim=1), nbins=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
